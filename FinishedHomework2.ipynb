{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: N-gram Language Models\n",
    "**Rishi Parida and Jazzy Howard**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Out: Thursday, February 20\n",
    "## Due Date: Sunday, March 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This programming assignment is more open-ended than the previous ones. It is centered on the N-gram language models and tasks you to:\n",
    "\n",
    "* download and process a large text dataset in python using the <code>csv</code> library\n",
    "* perform sentence and word tokenization\n",
    "* calculate N-gram counts and probabilities\n",
    "* compare the characteristics of the N-grams across different models\n",
    "* generate random sentences using the models\n",
    "\n",
    "<u>You may work in teams of two or three (2-tuples or 3-tuples?) for this assignment.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#Users/anjuchopra/Downloads/Wine/winemag-data_first150k.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Download two large text datasets from Kaggle.</u>\n",
    "\n",
    "The <a href=\"http://kaggle.com\">Kaggle competition hosting site</a> offers a number of free datasets that contain interesting text fields. For this assignment, we will use the \"Wine Reviews\" and \"All the News\" datasets. They can be accessed by selecting the \"Datasets\" header and then searching for these specific datasets. Then, choose \"Data\" from the sub-header, preview some of the csv data and notice how at least one of the columns in the dataset will contain sufficient text. I chose to direct you to these two datasets because the textual content seemed interesting and would have different language characteristics, and both were large csv files that could generate significant n-gram counts, but not be too large of a file.\n",
    "\n",
    "<em>(You can use other datasets if you wish. Others that looked interesting on Kaggle include the \"Yelp Dataset\" (but its over 3GB !!!), \"SMS Spam Collection Dataset\", \"Russian Troll Tweets\", and \"A Million News Headlines\".)</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Process the downloaded <code>csv</code> files in python.</u>\n",
    "\n",
    "There's a nice csv library already included in python for accessing values in that are stored in a comma separated values (csv) format. Read the <a href=\"https://docs.python.org/3/library/csv.html\">csv library documentation</a>.\n",
    "What is the delimiter in your csv files? Open each of the two .csv files that you downloaded using this library and be able to read in the data. Note that we really only care about the text column in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON CODE HERE\n",
    "#wine = '/Users/anjuchopra/Downloads/Wine/Testing2.csv'\n",
    "wine = '/Users/anjuchopra/Downloads/winemag-data-130k-v2.csv'\n",
    "debate = '/Users/anjuchopra/Downloads/debate_transcripts_v3_2020-02-26.csv'\n",
    "\n",
    "descriptions = []\n",
    "sentTokens = []\n",
    "wordTokens = []\n",
    "\n",
    "debateDescriptions = []\n",
    "debateSentTokens = []\n",
    "debateWordTokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Perform sentence segmentation and word tokenization.</u>\n",
    "\n",
    "Utilize the nltk module to perform sentence segmentation and word tokenization. But at this point, there are a few decisions that need to be made:\n",
    "\n",
    "* How we should handle the .csv rows in the previous step? If we ignore row makers, and \"lump everything together\", how will that effect our language model?\n",
    "* Do we want to remove punctuation? What is the effect of keeping punctuation in the model?\n",
    "* Do we want to add sentence boundary markers, such as <samp>&lt;S&gt;</samp> and <samp>&lt;/S&gt;</samp>?</li>\n",
    "* Should two the words <samp>The</samp> and <samp>the</samp> be treated as the same? What are the effects of doing, or not doing, this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/anjuchopra/Downloads/winemag-data-130k-v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2aab129235b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mreader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcurrDesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/anjuchopra/Downloads/winemag-data-130k-v2.csv'"
     ]
    }
   ],
   "source": [
    "with open(wine, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    reader\n",
    "    for row in reader:\n",
    "        currDesc = row['description']\n",
    "        descriptions.append(currDesc)        \n",
    "        \n",
    "        sentTokens.append(sent_tokenize(currDesc))\n",
    "        \n",
    "        currWord = word_tokenize(currDesc)\n",
    "        currWord = ['0.0' if is_number(x) else (x if x is 'i' else x) for x in currWord if (x.isalnum() or x == '.' or x == ',')]\n",
    "        \n",
    "        wordTokens.append(currWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(debate, encoding='mac_roman', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        currDesc = row['speech']\n",
    "        debateDescriptions.append(currDesc)        \n",
    "        \n",
    "        debateSentTokens.append(sent_tokenize(currDesc))\n",
    "        \n",
    "        currWord = word_tokenize(currDesc)\n",
    "        currWord = ['0.0' if is_number(x) else (x if x is 'i' else x) for x in currWord if (x.isalnum() or x == '.' or x == ',')]\n",
    "        #if(currWord.isalnum()):\n",
    "        #print(\"NEW WORD\", currWord)\n",
    "        debateWordTokens.append(currWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Calculate N-gram counts and compute probabilities.</u>\n",
    "\n",
    "Use a python dictionary (or any suitable data structure) to first compute unigram counts. Then try bigram counts. Finally, trigram counts.\n",
    "\n",
    "How much memory are you using? How fast, or slow, is the code -- how long is this step taking? If it is taking too long, try only using a fraction of your corpus: instead of loading the entire .csv file, try only reading the first 1000 rows of data.\n",
    "\n",
    "Using those counts, compute the probabilities for the unigrams, bigrams, and trigrams, and store those in a new python dictionary (or some other data structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(words_list, n):\n",
    "    ngrams_list = []\n",
    " \n",
    "    for num in range(0, len(words_list)):\n",
    "        ngram = ' '.join(words_list[num:num + n])\n",
    "        ngrams_list.append(ngram)\n",
    " \n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = {}\n",
    "bigrams = {}\n",
    "trigrams = {}\n",
    "\n",
    "unigramsTemp = []\n",
    "bigramsTemp = []\n",
    "trigramsTemp = []\n",
    "\n",
    "for i in wordTokens:\n",
    "    unigramsTemp.append(generate_ngrams(i, 1))\n",
    "    bigramsTemp.append(generate_ngrams(i, 2))\n",
    "    trigramsTemp.append(generate_ngrams(i, 3))\n",
    "#print (unigramsTemp)\n",
    "\n",
    "for c in unigramsTemp:\n",
    "    for i in c:\n",
    "        if i in unigrams:\n",
    "            unigrams[i] = unigrams[i] + 1\n",
    "        else:\n",
    "            unigrams[i] = 1\n",
    "\n",
    "for c in bigramsTemp:\n",
    "    for i in c:\n",
    "        if i in bigrams:\n",
    "            bigrams[i] = bigrams[i] + 1\n",
    "        else:\n",
    "            bigrams[i] = 1\n",
    "            \n",
    "for c in trigramsTemp:\n",
    "    for i in c:\n",
    "        if i in trigrams:\n",
    "            trigrams[i] = trigrams[i] + 1\n",
    "        else:\n",
    "            trigrams[i] = 1\n",
    "            \n",
    "\n",
    "unigramsList = sorted(unigrams.items(), key=lambda x: x[1], reverse=True)\n",
    "bigramsList = sorted(bigrams.items(), key=lambda x: x[1], reverse=True)\n",
    "trigramsList = sorted(trigrams.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debateUnigrams = {}\n",
    "debateBigrams = {}\n",
    "debateTrigrams = {}\n",
    "\n",
    "unigramsTemp = []\n",
    "bigramsTemp = []\n",
    "trigramsTemp = []\n",
    "\n",
    "for i in debateWordTokens:\n",
    "    unigramsTemp.append(generate_ngrams(i, 1))\n",
    "    bigramsTemp.append(generate_ngrams(i, 2))\n",
    "    trigramsTemp.append(generate_ngrams(i, 3))\n",
    "#print (unigramsTemp)\n",
    "\n",
    "for c in unigramsTemp:\n",
    "    for i in c:\n",
    "        if i in debateUnigrams:\n",
    "            debateUnigrams[i] = debateUnigrams[i] + 1\n",
    "        else:\n",
    "            debateUnigrams[i] = 1\n",
    "\n",
    "for c in bigramsTemp:\n",
    "    for i in c:\n",
    "        if i in debateBigrams:\n",
    "            debateBigrams[i] = debateBigrams[i] + 1\n",
    "        else:\n",
    "            debateBigrams[i] = 1\n",
    "\n",
    "for c in trigramsTemp:\n",
    "    for i in c:\n",
    "        if i in debateTrigrams:\n",
    "            debateTrigrams[i] = debateTrigrams[i] + 1\n",
    "        else:\n",
    "            debateTrigrams[i] = 1\n",
    "            \n",
    "\n",
    "debateUnigramsList = sorted(debateUnigrams.items(), key=lambda x: x[1], reverse=True)\n",
    "debateBigramsList = sorted(debateBigrams.items(), key=lambda x: x[1], reverse=True)\n",
    "debateTrigramsList = sorted(debateTrigrams.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "a = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNonStarter(base):\n",
    "    totWords = 0\n",
    "    possibleWords = []\n",
    "    \n",
    "    for i in bigramsList:\n",
    "        if(i[0].startswith(base + ' ')):\n",
    "            #add the total of this word to the total\n",
    "            totWords += i[1]\n",
    "            #put the reference as what the total is now (IF YOU NEED NO SPACE DELETE THE SPACE HERE)\n",
    "            possibleWords.append((i[0].replace(base + ' ',''), totWords))\n",
    "            #generate a random number between 0 and the total, the number will be the index of the word\n",
    "            \n",
    "    #print(possibleWords)\n",
    "    try:\n",
    "        rndIndex = random.randrange(totWords)\n",
    "    except:\n",
    "        try:\n",
    "            rndIndex = 0\n",
    "        except:\n",
    "            try:\n",
    "                generateNonStarter(random.choice(starters()))\n",
    "            except:\n",
    "                generateNonStarter(\"the\")\n",
    "    #print(rndIndex)\n",
    "    \n",
    "    for i in possibleWords:\n",
    "        if rndIndex < i[1]:\n",
    "            return i[0]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNonStarterTri(base):\n",
    "    totWords = 0\n",
    "    possibleWords = []\n",
    "    \n",
    "    for i in trigramsList[1:]:\n",
    "        threeWords = i[0].split()\n",
    "        if(len(threeWords) > 1):\n",
    "            \n",
    "            twoWords = threeWords[0] + \" \" + threeWords[1]\n",
    "            \n",
    "            if(twoWords == base):\n",
    "                #add the total of this word to the total\n",
    "                totWords += i[1]\n",
    "                #put the reference as what the total is now (IF YOU NEED NO SPACE DELETE THE SPACE HERE)\n",
    "                possibleWords.append((i[0].replace(base + ' ',''), totWords))\n",
    "       \n",
    "    try:\n",
    "        rndIndex = random.randrange(totWords)\n",
    "    except:\n",
    "        rndIndex = 0\n",
    "    #print(rndIndex)\n",
    "    \n",
    "    for i in possibleWords:\n",
    "        try: \n",
    "            if rndIndex < i[1]:\n",
    "                return i[0]\n",
    "                break\n",
    "        except:\n",
    "            return generateNonStarter(base.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDebateNonStarter(base):\n",
    "    totWords = 0\n",
    "    possibleWords = []\n",
    "    \n",
    "    for i in debateBigramsList:\n",
    "        if(i[0].startswith(base + ' ')):\n",
    "            #add the total of this word to the total\n",
    "            totWords += i[1]\n",
    "            #put the reference as what the total is now (IF YOU NEED NO SPACE DELETE THE SPACE HERE)\n",
    "            possibleWords.append((i[0].replace(base + ' ',''), totWords))\n",
    "            #generate a random number between 0 and the total, the number will be the index of the word\n",
    "            \n",
    "    #print(possibleWords)\n",
    "    try:\n",
    "        rndIndex = random.randrange(totWords)\n",
    "    except:\n",
    "        try:\n",
    "            rndIndex = 0\n",
    "        except:\n",
    "            try:\n",
    "                generateDebateNonStarter(random.choice(debateStarters()))\n",
    "            except:\n",
    "                generateDebateNonStarter(\"the\")\n",
    "    #print(rndIndex)\n",
    "    \n",
    "    for i in possibleWords:\n",
    "        if rndIndex < i[1]:\n",
    "            return i[0]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDebateNonStarterTri(base):\n",
    "    totWords = 0\n",
    "    possibleWords = []\n",
    "    \n",
    "    for i in debateTrigramsList[1:]:\n",
    "        threeWords = i[0].split()\n",
    "        if(len(threeWords) > 1):\n",
    "            \n",
    "            twoWords = threeWords[0] + \" \" + threeWords[1]\n",
    "            \n",
    "            if(twoWords == base):\n",
    "                #add the total of this word to the total\n",
    "                totWords += i[1]\n",
    "                #put the reference as what the total is now (IF YOU NEED NO SPACE DELETE THE SPACE HERE)\n",
    "                possibleWords.append((i[0].replace(base + ' ',''), totWords))\n",
    "       \n",
    "    rndIndex = random.randrange(totWords)\n",
    "    \n",
    "    for i in possibleWords:\n",
    "        if rndIndex < i[1]:\n",
    "            return i[0]\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Compare the statistics of the corpora.</u>\n",
    "                        \n",
    "Use the results of those calculations that you just made the poor computer painstakingly compute. What are the differences in the most common unigrams between the two language models? Are there interesting differences between the bigram models or trigram models?\n",
    "\n",
    "Be able to sort the n-grams to output the top k with the highest count or probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wine reviews: \",bigramsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Debate: \",debateBigramsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Generate random sentences from the N-grams models for both datasets.</u>\n",
    "                        \n",
    "We briefly talked about this idea in class. It's also introduced at a high-level in J&M 4.3. How can a random number in the range [0,1] probabilistically generate a word using your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWords(currWord):\n",
    "    sentence = \"\"\n",
    "    while not sentence.endswith('.'):\n",
    "        sentence = addWord(sentence, currWord)\n",
    "        currWord = generateNonStarter(currWord)\n",
    "        if currWord == None:\n",
    "            try:\n",
    "                currWord = generateNonStarter(random.choice(getStarter()))\n",
    "            except:\n",
    "                currWord = \"where\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWordsSize(currWord, length):\n",
    "    sentence = \"\"\n",
    "    for i in range(length):\n",
    "        sentence = addWord(sentence, currWord)\n",
    "        currWord = generateNonStarter(currWord)\n",
    "        if currWord == None:\n",
    "            try:\n",
    "                currWord = generateNonStarter(random.choice(getStarter()))\n",
    "            except:\n",
    "                currWord = \"where\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWordsTri(lastWord, currWord):\n",
    "    sentence = lastWord\n",
    "    while not sentence.endswith('.'):\n",
    "        sentence = addWord(sentence, currWord)\n",
    "        temp = currWord\n",
    "        currWord = generateNonStarterTri(lastWord + \" \" + currWord)\n",
    "        lastWord = temp\n",
    "        if currWord == None:\n",
    "            try:\n",
    "                currWord = generateNonStarter(random.choice(getStarter()))\n",
    "            except:\n",
    "                currWord = \"where\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDebateWords(currWord):\n",
    "    sentence = \"\"\n",
    "    while not sentence.endswith('.'):\n",
    "        sentence = addWord(sentence, currWord)\n",
    "        currWord = generateDebateNonStarter(currWord)\n",
    "        if currWord == None:\n",
    "            try:\n",
    "                currWord = generateDebateNonStarter(random.choice(getDebateStarter()))\n",
    "            except:\n",
    "                currWord = \"where\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDebateWordsSize(currWord, length):\n",
    "    sentence = \"\"\n",
    "    for i in range(length):\n",
    "        sentence = addWord(sentence, currWord)\n",
    "        currWord = generateDebateNonStarter(currWord)\n",
    "        if currWord == None:\n",
    "            try:\n",
    "                currWord = generateDebateNonStarter(random.choice(getDebateStarter()))\n",
    "            except:\n",
    "                currWord = \"where\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDebateWordsTri(lastWord, currWord):\n",
    "    sentence = lastWord\n",
    "    while not sentence.endswith('.'):\n",
    "        sentence = addWord(sentence, currWord)\n",
    "        temp = currWord\n",
    "        currWord = generateDebateNonStarterTri(lastWord + \" \" + currWord)\n",
    "        lastWord = temp\n",
    "        if currWord == None:\n",
    "            try:\n",
    "                currWord = generateNonStarter(random.choice(getStarter()))\n",
    "            except:\n",
    "                currWord = \"where\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWord(base, word):\n",
    "    if word == \"0.0\":\n",
    "        word = str(random.randrange(0,9999))\n",
    "    try:\n",
    "        return base + (\" \" if not word.endswith('.') and not word.endswith(',') and not base == \"\" else \"\") + (word if not word == 'i' else 'I')\n",
    "    except:\n",
    "        return base + \" \" + (word if not word == 'i' else 'I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDebateStarter():\n",
    "    return generateDebateNonStarter('.')\n",
    "\n",
    "def getStarter():\n",
    "    return generateNonStarter('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomStarter = random.choice(getStarter())\n",
    "\n",
    "def biWine():\n",
    "    sentence = generateWords(getStarter())\n",
    "    return(sentence)\n",
    "\n",
    "def triWine():\n",
    "    temp = getStarter()\n",
    "    try:\n",
    "        temp2 = generateWordsSize(temp,2).split()[1]\n",
    "    except:\n",
    "        temp2 = getStarter()\n",
    "    try:\n",
    "        sentence = generateWordsTri(temp, temp2)\n",
    "    except:\n",
    "        try:\n",
    "            sentence = generateWordsTri(temp, temp2)\n",
    "        except:\n",
    "            print(\"Unexpected error\")\n",
    "    return(sentence)\n",
    "\n",
    "\n",
    "def biDebate():\n",
    "    debateSentence = generateDebateWords((getDebateStarter()))\n",
    "    return(debateSentence)\n",
    "\n",
    "def triDebate():\n",
    "    temp = getDebateStarter()\n",
    "    try:\n",
    "        temp2 = generateDebateWordsSize(temp,2).split()[1]\n",
    "    except:\n",
    "        temp2 = getStarter()\n",
    "    try:\n",
    "        sentence = generateDebateWordsTri(temp, temp2)\n",
    "    except:\n",
    "        try:\n",
    "            sentence = generateDebateWordsTri(temp, temp2)\n",
    "        except:\n",
    "            print(\"Unexpected error!\")\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a technical report (in this Jupyter Notebook, with good Markdown formatting) that documents your findings, \"lessons learned\", any areas of where you ran into difficult, and also any other interesting details. Include in your report the following details:\n",
    "\n",
    "1. Names of the datasets used.\n",
    "1. Does your model use all of the data in the .csv file or only a subset of it (i.e. first 1,000 rows)?\n",
    "1. What is the vocabulary and size of each dataset?\n",
    "1. How did you handle the merging of separate rows in a .csv file? How did you handle sentence segmentation with sentence boundary markers? Also report on any other decisions made in step #3.\n",
    "1. How long did it take your program to build these models? Do you have any statistics on memory/RAM usage?\n",
    "1. Output the top 15 unigrams, bigrams, trigrams for each model. Are there any interesting differences?\n",
    "1. Output 3 different randomly generated sentences for each unigram, bigram, trigram model. How did you know where the randomly generated sentence ended?\n",
    "\n",
    "Also submit this python notebook `.ipynb` to D2L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Names of the datasets used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets I used were the \"Wine Reviews\" (included in instructions) and the 2020 Democratic Debate Transcripts (can be found here):\n",
    "https://www.kaggle.com/brandenciranni/democratic-debate-transcripts-2020 I used the former since it was in the instructions, but I decided to go with something a little different for the second data set. I settled on the 2020 Democratic Debate Transcripts since I thought it would be interesting to generate sentences based on politician's speaking patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Does your model use all of the data in the .csv file or only a subset of it (i.e. first 1,000 rows)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My models both use all of the data in the .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is the vocabulary and size of each dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wine reviews set size:\",len(unigrams))\n",
    "print(\"Debate set size:\",len(debateUnigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the vocabulary for the wine reviews is **35186**\n",
    "The size of the vocabulary for the debate transcripts is **10063**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How did you handle the merging of separate rows in a .csv file? How did you handle sentence segmentation with sentence boundary markers? Also report on any other decisions made in step #3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the datasets I stored each row as a separate item in a list, then used a nested for loop to combine all the rows for unigrams, bigrams, and trigrams. Because of this, sentence segmentation markers weren't necessary, since they were all stored in different items. I decided to keep both the puntuation marks `,` and `.` as separate words, since they do change the meaning of sentences often. All other punctuation is discarded. Additionally, capital and lowercase words are treated as different words, so *The* and *the* are two different words since they will often have to deal with different contexts. All numbers were changed to be equal to 0.0, so n-gram models would just read them as a number would be following/preceding the target word. Just for a bit of extra fun, I generated a random number from 0-9999 when a number would be outputted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How long did it take your program to build these models? Do you have any statistics on memory/RAM usage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately I don't know how to get any statistics on memory/RAM usage. It usually takes about 80-120 seconds to load the data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Output the top 15 unigrams, bigrams, trigrams for each model. Are there any interesting differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wine unigrams:\", unigramsList[:15])\n",
    "print(\"Wine bigrams:\", bigramsList[:15])\n",
    "print(\"Wine trigrams:\", trigramsList[:15])\n",
    "\n",
    "print(\"\\n\\nDebate unigrams:\", debateUnigramsList[:15])\n",
    "print(\"Debate bigrams:\", debateBigramsList[:15])\n",
    "print(\"Debate trigrams:\", debateTrigramsList[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigrams, bigrams, and trigrams are all (as expected) very different. While the wine n-grams are very focused on smells, aromas, and descriptions, the debate n-grams are all person and action focused, with words like I and we being much more heavily emphasized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output 3 different randomly generated sentences for each unigram, bigram, trigram model. How did you know where the randomly generated sentence ended?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"***Bigram with Wine Review dataset***\\n\")\n",
    "for i in range(3):\n",
    "    print(biWine())\n",
    "\n",
    "print(\"\\n***Trigram with Wine Review dataset***\\n\")\n",
    "for i in range(3):\n",
    "    print(triWine())\n",
    "\n",
    "print(\"\\n***Bigram with Debate dataset***\\n\")\n",
    "for i in range(3):\n",
    "    print(biDebate())\n",
    "    \n",
    "print(\"\\n***Trigram with Debate dataset***\\n\")\n",
    "for i in range(3):\n",
    "    print(triDebate())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The randomly generated sentences end when they reach a `.` character."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
